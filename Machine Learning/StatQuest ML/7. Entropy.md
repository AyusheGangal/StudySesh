- Entropy is used in a lot of Data Science. It can be used to build Classification Trees, which are used to classify things.
- Entropy is also the basis of [[Mutual Information]] which quantifies the relationship between two things.
- Entropy is the basis of **Relative Entropy** (aka **The Kullback-Leibler Distance**) and **Cross Entropy**, which shows up all over the place, including fancy dimension reduction algorithms like t-SNE and UMAP.![[Screen Shot 2024-12-22 at 22.00.15 PM.png]]
- All these things use Entropy, or something derived from it, to quantify **similarities** and **differences**.

### [[Surprise]]:
Surprise is inversely related to probability. Example, if a box has 4 blue balls and 1 red ball, and we picked a blue ball randomly, we would not be surprised. but if we pick a red ball, we would be relatively surprised.

In other words, when the probability of picking up a red ball is low, the surprise is high. And when the probability of picking up a red ball is high, the surprise is low.

Surprise is calculated by taking the inverse of the log of the probability. 
![[Screen Shot 2024-12-22 at 22.15.45 PM.png|400]]

Now since probability of getting heads is 1 since we are always getting heads in this example, the surprise for heads will be 0.
![[Screen Shot 2024-12-22 at 22.17.25 PM.png|400]]

In contrast, since the prob for getting tails is 0, and thus we will never get tails, it does not make sense to quantify the surprise of something that will never happen.
- Using properties of log to turn division into subtraction, we get log(1) - log(0).
- As log(0) is not defined, the whole thing becomes undefined.![[Screen Shot 2024-12-22 at 22.20.04 PM.png|400]]

Curve for Surprise:
![[Screen Shot 2024-12-22 at 22.21.40 PM.png|400]]
- The closer the probability gets to 0, the more surprise we get.
- There is no surprise when the probability is 1.
- Surprise is the log of the inverse of the probability.

>[!Note] Use log base 2 for calculations when calculating surprise for 2 outputs
>![[Screen Shot 2024-12-22 at 22.24.40 PM.png|400]]
>Example, if outputs are heads and tails.

Example, if our coin gets heads 90% of the times and tails 10% of the times. Calculating surprise for both, we will find that because getting tails is much rarer then heads, the surprise for tails in much larger.
![[Screen Shot 2024-12-22 at 22.28.12 PM.png|500]]

### Case: Flipping the coin 3 times
P(Heads) = 0.9
P(Tails) = 0.1

Event: Heads - Heads - Tails
P(Event) =  0.9 x 0.9 x 0.1

Surprise(Event) = $\frac{1}{log(0.9 * 0.9 * 0.1)}$

![[Screen Shot 2024-12-23 at 17.07.31 PM.png|400]]

- We see that the total Surprise for a sequence of coin tosses is just the sum of the Surprises for each individual toss.

### Case: coins if flipped 100 times
- We multiply the Probabilities with 100 to find the expected number of heads and tails.
- ![[Screen Shot 2024-12-23 at 17.11.40 PM.png|400]]

If we divide everything by the total number of coin tosses, we get the average amount of Surprise per coin, which will be 0.47 here. Which means, on average we expect the Surprise to be 0.47 every time we flip the coin. 

This is called the **Entropy** of the coin. (The average or Expected Surprise per time we flip the coin)
Entropy is the Expected value of the Surprise:
![[Screen Shot 2024-12-23 at 17.19.33 PM.png|400]]

Written using Sigma notation:
![[Screen Shot 2024-12-23 at 17.22.41 PM.png|500]]
- Expected value of Surprise is the sum of product of specific surprise values with the probability of observing that specific value for Surprise. 

After plugging in the formula for Surprise, which is $log(\frac{1}{p(x)})$ , and we can plug in the probability value for that Surprise, which will be $p(x)$. We get,
![[Screen Shot 2024-12-23 at 17.28.18 PM.png|400]]

Derivation of Entropy formula which was first published by Claude Shannon in 1948:
![[Screen Shot 2024-12-23 at 17.35.49 PM.png]]

### Entropy for example with 7 chickens
Given 6 chickens are orange, and 1 chicken is blue.
![[Screen Shot 2024-12-23 at 17.53.15 PM.png|400]]

Even though Surprise for picking orange chicken is much lower than surprise for picking blue chickens, the probability of picking orange chicken is much higher than the probability of picking the blue chickens. Thus, Total Entropy of 0.59 is much closer to the Surprise associated with orange chickens (0.22) than blue chickens (2.81) 

Entropy would be 1 for an area which has an equal number of orange and blue chickens (7 each).
![[Screen Shot 2024-12-23 at 17.58.14 PM.png|400]]

As a result, we can use Entropy to quantify the similarity or difference in the number of orange or blue chickens in each area.

>[! Summary]
>- Entropy is highest when we have the same number of chickens.
>- As we increase the difference in the number of orange or blue chickens, we lower the Entropy.

