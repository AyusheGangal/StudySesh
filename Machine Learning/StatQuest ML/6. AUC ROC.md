Starting with some data on mice. The blue dots representing "Obese" mice and red dots representing "Not Obese" mice Along the X-axis, we have weight of the mice. 

![[Screen Shot 2024-12-22 at 14.45.08 PM.png|400]]
![[Screen Shot 2024-12-22 at 14.45.57 PM.png|400]]

### Fitting a Logistic regression curve to this data
![[Screen Shot 2024-12-22 at 14.52.39 PM.png|400]]

- When we implement Logistic Regression, the Y-axis is converted to the probability that a mouse is obese or not. 
- Logistic Regression is a Classification algorithm, which tells us the probability of whether the mouse if obese or not based on its weight (here).![[Screen Shot 2024-12-22 at 14.55.01 PM.png|400]]

- Therefore, we can use the logistic regression curve to tell us the "Probability" that a mouse is obese or not based on its weight.
	![[Screen Shot 2024-12-22 at 14.56.45 PM.png|400]]

However, if we want to classify the mice as "Obese" or "Not Obese", then we need a way to turn these probabilities into classifications (predictions).
- One way to classify mice is **set a threshold** value.

### Setting threshold of 0.5
- ![[Screen Shot 2024-12-22 at 15.00.25 PM.png|400]]
	And classify all mice as "Obese" with a probability of being obese > 0.5 as "Obese". 
	And classify all mice as "Not Obese" with a probability of being not obese <= 0.5 as "Not Obese".

To evaluate the effectiveness of the Logistic Regression with a classification threshold set to 0.5, we can test it with mice that we know are obese or not obese. 
![[Screen Shot 2024-12-22 at 15.05.56 PM.png|400]]

Creating a confusion matrix to summarize the classifications,![[Screen Shot 2024-12-22 at 15.07.05 PM.png|400]]

Using this confusion matrix to calculate sensitivity and specificity to evaluate Logistic Regression with classification threshold of 0.5 for obesity.

### Setting threshold of 0.1
- If we were to use a classification threshold of 0.1 instead, we would have correct classifications for all 4 obese mice, but will also increase the number of false positives. 
- The lower threshold would also reduce the number of False Negatives, because all the obese mice were correctly classified.
	![[Screen Shot 2024-12-22 at 15.22.15 PM.png|400]]

### Setting threshold of 0.9
![[Screen Shot 2024-12-22 at 15.24.33 PM.png|400]]

- In this case, we would correctly classify the same number of obese samples as when the threshold was set to 0.5, but we would not have False-Positives. 
- And we would correctly classify one more sample that was not obese.
- Also will have the same number of False-Negatives as before.
- ![[Screen Shot 2024-12-22 at 15.27.59 PM.png|400]]

With this data, we find that a higher threshold does a better job classifying samples as obese or not obese.

### How to determine which threshold value to choose
- We don't need to test every single option, as some thresholds might result in the same confusion matrix.
- But even if we made one confusion matrix for every single threshold that mattered, it would still be too large a number of confusion matrices.

Solution is to use: 
**Receiver Operator Characteristic** ([[ROC]]) graphs which provide a simple way to summarize all of the information.
![[Screen Shot 2024-12-22 at 15.33.30 PM.png|400]]

- The Y-axis shows the True Positive Rate (TPR) which is the same as Sensitivity.
	- ![[Screen Shot 2024-12-22 at 15.35.58 PM.png|400]]
	- True Positive Rate tells you what proportion of Obese (positive) samples were correctly classified.
- The X-axis shows the False Positive Rate (FPR) which is (1-Specificity).
	- ![[Screen Shot 2024-12-22 at 15.39.20 PM.png|400]]
	- False Positive Rate tells you the proportion of not obese samples that were incorrectly classified and are False Positives.

### ROC from start to finish for better understanding
Using the same sample data ![[Screen Shot 2024-12-22 at 16.36.57 PM.png]]

- Starting by using a threshold that classifies all of the samples as "Obese" (positive in our case), and we create a confusion matrix using this.![[Screen Shot 2024-12-22 at 16.39.17 PM.png|500]]
- Calculating the True Positive Rate or Sensitivity using this confusion matrix, $$Sensitivity = \frac{4}{4+0} = 1$$
- The True Positive Rate when the threshold is so low that every sample is classified as obese is 1. **This means that every single obese sample was classified correctly.**
- Now calculating the False Positive Rate or (1-Specificity),$$1-Specificity = \frac{4}{4+0} = 1$$
- The False Positive Rate when the threshold is so low that every sample is classified as obese is also 1. **This means that every single sample that was "not obese" was incorrectly classified as "obese"**

- Plotting a point at (1,1): Means that even though we correctly classified all "Obese" samples correctly, we incorrectly classified all of the samples that were "not obese".![[Screen Shot 2024-12-22 at 16.48.10 PM.png]]

#### What does it mean when TPR = FPR
- The green diagonal line represents where the **True Positive Rate = False Positive Rate**![[Screen Shot 2024-12-22 at 16.50.40 PM.png|400]]
- Any point on this green diagonal line means that the proportion of correctly classified "obese" samples is the same as the proportion of incorrectly classified samples that are "not obese".

Repeating the same with different threshold values will result in obtaining different confusion matrices, and therefore, different points on the graph.

For instance, if we get a new point (0.75, 1): it means that it is to the left of the dotted green diagonal line, we know that the proportion of correctly classified "obese" samples (true positives) is greater than the proportion of the samples that were incorrectly classified "obese" false positives).
	![[Screen Shot 2024-12-22 at 17.03.22 PM.png|300]]

- In other words, this new threshold for deciding whether a sample is obese or not is better than the previous one.

Repeating it again using a different threshold, and obtaining a new point (0.5, 1) on the graph. This new point is even further left of the green dotted line, showing that new threshold further decreases the proportion of samples that were incorrectly classified as "obese" (false positives)
![[Screen Shot 2024-12-22 at 17.07.49 PM.png|300]]

Repeating it again and again, suppose we get a point (0, 0.75): This threshold will mean that we classified 75% of the "obese" samples correctly, and correctly classified 100% of the samples that were "not obese".
![[Screen Shot 2024-12-22 at 17.10.33 PM.png|300]]
- In other words, this threshold resulted in **No False Positives**.

This is the plot we get after we try all of the threshold values,
![[Screen Shot 2024-12-22 at 17.12.35 PM.png|400]]
- The point (0, 0) represents a threshold value that incorrectly classified all samples that were "obese" or results in zero (0) True Positives, and correctly classified all samples that were "not obese" or resulted in zero (0) False Positives.

We can connect these dots to obtain the ROC graph.
![[Screen Shot 2024-12-22 at 17.15.28 PM.png|400]]
- The ROC graph summarizes all of the confusion matrices that each threshold produced.

- Without having to sort through the confusion matrices, we can tell that the threshold that corresponds to point (0, 0.75) is better than the threshold that corresponds to the one its right.
	![[Screen Shot 2024-12-22 at 17.18.38 PM.png|400]]

- Also, depending on how many False Positives we are willing to accept, the optimal threshold is either: 
	![[Screen Shot 2024-12-22 at 17.20.37 PM.png|400]]


The Area under the Curve (**AUC**)
![[Screen Shot 2024-12-22 at 17.22.31 PM.png|400]]

- The AUC makes it easy to compare one ROC curve to another.
- For example, the AUC for the red ROC curve is greater than the AUC for the blue ROC curve, suggesting that the red curve is better.![[Screen Shot 2024-12-22 at 17.23.48 PM.png|400]]
- So if the red ROC curve represented Logistic Regression, and the blue ROC curve represented Random Forest, you should choose Logistic Regression.

>[!Tip]
>Although, ROC graphs are drawn using True Positive Rates and False Positive Rates to summarize confusion matrices, there are other metrics that attempt to do the same thing.
>
>For example, Precision is often used in place of False Positive Rate 

[[Precision]] is defined as,
![[Screen Shot 2024-12-22 at 17.29.52 PM.png|300]]

>[! Tip] Why is precision more useful in some cases
>- Therefore, precision is the proportion of positive results that were correctly classified.
>- If there were lots of samples that were "not obese" relative to the number of "obese" samples, then Precision might be more useful than **False Positive Rate**.
>- This is because Precision does not include the number of **True Negatives** in its calculation, and is not effected by the imbalance. 
>- In practice, this imbalance usually occurs when studying a rare disease. In this case, the study will contain many people without the disease than with the disease.

>[!Example] In summary,
>ROC curve makes it easy to identify the best threshold for making a decision, and the AUC can help you decide which categorization (algorithm) is better.

