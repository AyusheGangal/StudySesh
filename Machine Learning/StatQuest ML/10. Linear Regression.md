### Main Ideas:
1. The first thing you do in linear regression is to use "Least Squares" to fit a line to the data.
2. The second thing you do is calculate $R^2$ (coefficient of determination)
3. Then you calculate a $p$-value for $R^2$.
	![[Screen Shot 2024-12-23 at 20.49.07 PM.png|400]]

After applying Least Squares to obtain the optimal fit for the mouse size to weight data, we get an equation $y = 0.1 + 0.78x$ , where 0.1 is the y-intercept, and 0.78 is the slope.
	- Since the slope is not 0, it means that knowing a mouse's weight will help us make a guess about that mouse's size. 
	- But how good is that guess?
		- **Calculating $R^2$ is the first step in determining how good that guess is.** 

For $R^2$:
- First, calculate the average mouse size. We have shifted all of the data points to the y-axis to show that at this point, we are only interested in mouse size.
	![[Screen Shot 2024-12-23 at 20.56.21 PM.png|300]]
	- this is the average mouse size:
		![[Screen Shot 2024-12-23 at 20.57.12 PM.png|300]]
		
- Now we sum the squared residuals for this average line:
		![[Screen Shot 2024-12-23 at 20.58.48 PM.png|400]]
		- This is called as "SS(mean)" for "**Sum of Squares around Mean**"
		- Note: 
			- **SS(mean) = (data - mean)$^2$** 
			- Variation around the mean = $\frac{(data - mean)^2}{n}$
				- where $n$ is the sample size
				- **Var(mean) =** $\frac{SS(mean)}{n}$
		
	![[Screen Shot 2024-12-23 at 21.03.40 PM.png|400]]
	- Another way to think about variance is as the average sum of squares per mouse.


Going back to the original least squares plot to find the sum of squared residuals around the least squares fit, and the variance for this fit:
![[Screen Shot 2024-12-23 at 21.10.20 PM.png|500]]

In general, variance is defines as the (Sum of Squares/The number of those things), which is also equal to the "Average sum of Squares":
![[Screen Shot 2024-12-23 at 21.11.48 PM.png]]

So, recapping a bit:
- We compare the raw variation in the mouse size for the average and the least squares line![[Screen Shot 2024-12-23 at 21.14.25 PM.png|500]]
- We find that there is **less variation around the line that we fit using least squares. That is, the residuals are smaller.** 
	![[Screen Shot 2024-12-23 at 21.16.00 PM.png|500]]
- As a result, we can say that some of the variation in the mouse size is "explained" by taking mouse weight into account.
- In other words, heavier mice are bigger; lighter mice are smaller.

>[! Example] R$^2$ Explained
>$R^2$ tells us how much of the variation in the mouse size can be explained by taking mouse weight into account.
>
>![[Screen Shot 2024-12-23 at 21.23.57 PM.png|200]]

### Example: Least Square Fit

If Var(mean) = 11.1 
If Var(fit) = 4.4
	Then R$^2$ = $\frac{11.1 - 4.4}{11.1} = 0.6$
	$\therefore R^2 = 0.6 = 60\%$
	$\implies$ There is a 60% reduction in variance when we take the mouse weight into account.
	- Alternatively, we can say that mouse weight "explains" 60% of the variation in mouse size.

![[Screen Shot 2024-12-23 at 21.30.36 PM.png]]

- We can also use sum of squares to make the same calculation. 
	- If given SS(mean) = 100; SS(fit) = 40
	- $R^2 = \frac{SS(mean)-SS(fit)}{SS(mean)} = \frac{100-40}{100} = 0.60 = 60\%$
$\implies$ that 60% of the sums of squares of the mouse size can be explained by mouse weight.

### Example: Line fits data perfectly
- When the line fits the data perfectly, the residuals = 0
	- Which implies, that the Variance(fit) = 0 as well.
	- $\therefore R^2 = 100\%$  
	
![[Screen Shot 2024-12-24 at 13.23.23 PM.png]]

### Example: Horizontal Fit with all options equally likely
- In this case, knowing mouse weight does not help us predict mouse size
- Here, a heavy mouse could be small or large; similarly, a light mouse could be big or small as well. **Each of those options are equally likely**
	![[Screen Shot 2024-12-24 at 13.26.52 PM.png|300]]
	- Just like mean, the variation around fit is also 11.1. Therefore, $R^2 = 0 = 0\%$
	![[Screen Shot 2024-12-24 at 13.30.32 PM.png]]

### Calculating $R^2$ general rule:
![[Screen Shot 2024-12-24 at 13.33.06 PM.png|400]]

### Complicated example:
Imagine we wanted to know if mouse weight and tail length did a good job predicting the length of the mouse's body. And plot the data using a 3D graph.
![[Screen Shot 2024-12-24 at 13.35.22 PM.png|400]]

- This is one of the data points on the 3D plot.
	![[Screen Shot 2024-12-24 at 13.37.21 PM.png|400]]

- This is how all of the data looks like on the 3D plot.
	![[Screen Shot 2024-12-24 at 13.38.36 PM.png|400]]
	- The larger circles are points which are closer to us, and represent mice that have shorter tails.
	- The smaller circles are points which are farther from us, and represent mice with longer tails.

We do a least squares fit, and since we have this added dimension, we will fit a plane to the data instead of a line.
	![[Screen Shot 2024-12-24 at 13.41.19 PM.png|400]]
	
- We can use the equation: $$y = 0.1 + 0.7x + 0.5z$$ to fit the curve. Where, $y$ represents the "body length" 
- The Least square estimates 3 parameters here:
	- The y-intercept = 0.1 (when mouse tail length and mouse weight are both 0)
	- 0.7 for the mouse weight
	- 0.5 for the tail length
	
- Now we can again measure residuals, square them up and add them to find $R^2$ 
	
- If the tail length (z-axis) is useless and does not make the SS(fit) smaller, then least squares will ignore it by making that parameter = 0
	- And in this case, plugging the tail length into the fit equation will have no effect on predicting the mouse size.
		![[Screen Shot 2024-12-24 at 14.02.07 PM.png|400]]

- This means equations with more parameters will not make SS(fit) worse than equations with fewer parameters.![[Screen Shot 2024-12-24 at 14.04.38 PM.png]]

- But more parameters we add to the equation, the more opportunities we have for random events to happen, which can reduce SS(fit) and result in a better $R^2$ value.
- Thus, people report an "adjusted $R^2$" value that scales $R^2$ by the number of parameters.

### Case where you only have 2 data points
- In this case $R^2$ is not enough
- Here, we have SS(mean) = 10; SS(fit) = 0![[Screen Shot 2024-12-24 at 14.11.25 PM.png]]
	- The SS(fit) is 0 as we can always draw a straight line to connect any two points.
	- Using this, we will get $R^2 = 100\%$ , but any two points will give us the same thing. 
	![[Screen Shot 2024-12-24 at 14.12.58 PM.png]]

$\therefore$ we need a way to determine if the $R^2$ value is statistically significant. Which is the [[p-value]].

Recap:
![[Screen Shot 2024-12-24 at 14.16.48 PM.png]]

