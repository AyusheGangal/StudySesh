### Main Ideas:
1. The first thing you do in linear regression is to use "Least Squares" to fit a line to the data.
2. The second thing you do is calculate $R^2$ (coefficient of determination)
3. Then you calculate a $p$-value for $R^2$.
	![[Screen Shot 2024-12-23 at 20.49.07 PM.png|400]]

After applying Least Squares to obtain the optimal fit for the mouse size to weight data, we get an equation $y = 0.1 + 0.78x$ , where 0.1 is the y-intercept, and 0.78 is the slope.
	- Since the slope is not 0, it means that knowing a mouse's weight will help us make a guess about that mouse's size. 
	- But how good is that guess?
		- **Calculating $R^2$ is the first step in determining how good that guess is.** 

For $R^2$:
- First, calculate the average mouse size. We have shifted all of the data points to the y-axis to show that at this point, we are only interested in mouse size.
	![[Screen Shot 2024-12-23 at 20.56.21 PM.png|300]]
	- this is the average mouse size:
		![[Screen Shot 2024-12-23 at 20.57.12 PM.png|300]]
		
- Now we sum the squared residuals for this average line:
		![[Screen Shot 2024-12-23 at 20.58.48 PM.png|400]]
		- This is called as "SS(mean)" for "**Sum of Squares around Mean**"
		- Note: 
			- **SS(mean) = (data - mean)$^2$** 
			- Variation around the mean = $\frac{(data - mean)^2}{n}$
				- where $n$ is the sample size
				- **Var(mean) =** $\frac{SS(mean)}{n}$
		
	![[Screen Shot 2024-12-23 at 21.03.40 PM.png|400]]
	- Another way to think about variance is as the average sum of squares per mouse.


Going back to the original least squares plot to find the sum of squared residuals around the least squares fit, and the variance for this fit:
![[Screen Shot 2024-12-23 at 21.10.20 PM.png|500]]

In general, variance is defines as the (Sum of Squares/The number of those things), which is also equal to the "Average sum of Squares":
![[Screen Shot 2024-12-23 at 21.11.48 PM.png]]

So, recapping a bit:
- We compare the raw variation in the mouse size for the average and the least squares line![[Screen Shot 2024-12-23 at 21.14.25 PM.png|500]]
- We find that there is **less variation around the line that we fit using least squares. That is, the residuals are smaller.** 
	![[Screen Shot 2024-12-23 at 21.16.00 PM.png|500]]
- As a result, we can say that some of the variation in the mouse size is "explained" by taking mouse weight into account.
- In other words, heavier mice are bigger; lighter mice are smaller.

>[! Example] R$^2$ Explained
>$R^2$ tells us how much of the variation in the mouse size can be explained by taking mouse weight into account.
>
>![[Screen Shot 2024-12-23 at 21.23.57 PM.png|200]]

### Example: Least Square Fit

If Var(mean) = 11.1 
If Var(fit) = 4.4
	Then R$^2$ = $\frac{11.1 - 4.4}{11.1} = 0.6$
	$\therefore R^2 = 0.6 = 60\%$
	$\implies$ There is a 60% reduction in variance when we take the mouse weight into account.
	- Alternatively, we can say that mouse weight "explains" 60% of the variation in mouse size.

![[Screen Shot 2024-12-23 at 21.30.36 PM.png]]

- We can also use sum of squares to make the same calculation. 
	- If given SS(mean) = 100; SS(fit) = 40
	- $R^2 = \frac{SS(mean)-SS(fit)}{SS(mean)} = \frac{100-40}{100} = 0.60 = 60\%$
$\implies$ that 60% of the sums of squares of the mouse size can be explained by mouse weight.

### Example: Line fits data perfectly
- When the line fits the data perfectly, the residuals = 0
	- Which implies, that the Variance(fit) = 0 as well.
	- $\therefore R^2 = 100\%$  
	
![[Screen Shot 2024-12-24 at 13.23.23 PM.png]]

### Example: Horizontal Fit with all options equally likely
- In this case, knowing mouse weight does not help us predict mouse size
- Here, a heavy mouse could be small or large; similarly, a light mouse could be big or small as well. **Each of those options are equally likely**
	![[Screen Shot 2024-12-24 at 13.26.52 PM.png|300]]
	- Just like mean, the variation around fit is also 11.1. Therefore, $R^2 = 0 = 0\%$
	![[Screen Shot 2024-12-24 at 13.30.32 PM.png]]

### Calculating $R^2$ general rule:
![[Screen Shot 2024-12-24 at 13.33.06 PM.png|400]]

### Complicated example:
Imagine we wanted to know if mouse weight and tail length did a good job predicting the length of the mouse's body. And plot the data using a 3D graph.
![[Screen Shot 2024-12-24 at 13.35.22 PM.png|400]]

- This is one of the data points on the 3D plot.
	![[Screen Shot 2024-12-24 at 13.37.21 PM.png|400]]

- This is how all of the data looks like on the 3D plot.
	![[Screen Shot 2024-12-24 at 13.38.36 PM.png|400]]
	- The larger circles are points which are closer to us, and represent mice that have shorter tails.
	- The smaller circles are points which are farther from us, and represent mice with longer tails.

We do a least squares fit, and since we have this added dimension, we will fit a plane to the data instead of a line.
	![[Screen Shot 2024-12-24 at 13.41.19 PM.png|400]]
	
- We can use the equation: $$y = 0.1 + 0.7x + 0.5z$$ to fit the curve. Where, $y$ represents the "body length" 
- The Least square estimates 3 parameters here:
	- The y-intercept = 0.1 (when mouse tail length and mouse weight are both 0)
	- 0.7 for the mouse weight
	- 0.5 for the tail length
	
- Now we can again measure residuals, square them up and add them to find $R^2$ 
	
- If the tail length (z-axis) is useless and does not make the SS(fit) smaller, then least squares will ignore it by making that parameter = 0
	- And in this case, plugging the tail length into the fit equation will have no effect on predicting the mouse size.
		![[Screen Shot 2024-12-24 at 14.02.07 PM.png|400]]

- This means equations with more parameters will not make SS(fit) worse than equations with fewer parameters.![[Screen Shot 2024-12-24 at 14.04.38 PM.png]]

- But more parameters we add to the equation, the more opportunities we have for random events to happen, which can reduce SS(fit) and result in a better $R^2$ value.
- Thus, people report an "adjusted $R^2$" value that scales $R^2$ by the number of parameters.

### Case where you only have 2 data points
- In this case $R^2$ is not enough
- Here, we have SS(mean) = 10; SS(fit) = 0![[Screen Shot 2024-12-24 at 14.11.25 PM.png]]
	- The SS(fit) is 0 as we can always draw a straight line to connect any two points.
	- Using this, we will get $R^2 = 100\%$ , but any two points will give us the same thing. 
	![[Screen Shot 2024-12-24 at 14.12.58 PM.png]]

$\therefore$ we need a way to determine if the $R^2$ value is statistically significant. Which is the [[p-value]].

Recap:
![[Screen Shot 2024-12-24 at 14.16.48 PM.png]]

The $p$-value for R$^2$ comes from something called "F", which is (variation in mouse size explained by weight/ variation in mouse size not explained by weight)
![[Screen Shot 2025-01-05 at 22.03.21 PM.png|400]]

The numerators for R$^2$ and F are the same, which is the reduction in variance when we take weight into account. 
The denominator of F can be explained by![[Screen Shot 2025-01-05 at 22.06.12 PM.png]]

F tells us if R$^2$ is significant or not
![[Screen Shot 2025-01-05 at 22.07.39 PM.png|300]]

"Degrees of freedom" turn the sum of squares into variances.
![[Screen Shot 2025-01-05 at 22.09.02 PM.png|300]]

$p_{fit}$ is the number of parameters in the fit line, and $p_{mean}$ is the number of parameters in the mean line.
![[Screen Shot 2025-01-05 at 22.13.24 PM.png]]

Both equations have a parameter for the y-intercept. However, the "fit" line has one extra parameter, the slope. Here, slope is the relationship between weight and size.
![[Screen Shot 2025-01-05 at 22.15.23 PM.png]]

The numerator is the variance explained by the extra parameter
![[Screen Shot 2025-01-05 at 22.16.13 PM.png|500]]

For the denominator, it is the sum of squares of residuals that remain after fitting the data.
Why divide by (n-$p_{fit}$) and not just n
![[Screen Shot 2025-01-05 at 22.22.18 PM.png]]

If the fit is good, then we have a really large value of "F"
![[Screen Shot 2025-01-05 at 22.24.03 PM.png]]

Conceptually, if we randomly generate data and find F values for it. and plot those F values on a histogram, and we do this multiple times
![[Screen Shot 2025-01-05 at 22.26.59 PM.png|200]]

Now if we return to our original dataset, and we get F = 6. 
The $p$-value is the number of more extreme values divided by all the values.
![[Screen Shot 2025-01-05 at 22.28.50 PM.png|300]]
- In this case, we have F=6 and F=7 divided by all other randomizations that we created originally. 

You can approximate this histogram using a line, but in practice, people use the line to calculate $p$-value instead of randomly generating tons of data.
![[Screen Shot 2025-01-05 at 22.32.20 PM.png|400]]

### Two F-distribution
Blue and red lines are different standard F distribution that people use to calculate $p$-values
Note: The degrees of freedom determine the shape
![[Screen Shot 2025-01-05 at 22.33.08 PM.png|400]]

- in this case, the sample size used to draw the red line is smaller than the sample size used to draw the blue line.

Te $p$-value is smaller when there are more samples relative to the number of parameters in the fit
![[Screen Shot 2025-01-05 at 22.36.48 PM.png]]

>[!Summary]
>Given some data that you think are related, then use Linear regression to:
>- Quantifies the relationship in the data (called as R$^2$ , which needs to be large).
>- Determines how reliable that relationship is (This is the p-value that we calculate using F)(the p-value needs to be small)








