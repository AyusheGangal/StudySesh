Bias: The inability for a machine learning method to capture the true relationship is called [[Bias]]. Variance: The difference in fits between data sets is called [[Variance]].

>[!Example] Example: Measuring weight and height of mice. 
>Given this data, we would like to predict mouse height given its weight. 
>
![[Screen Shot 2024-12-22 at 13.49.08 PM.png|400]]
> ![[Screen Shot 2024-12-22 at 13.50.43 PM.png|400]]

But here, we do not know the formula so we use two machine learning algorithms to approximate this relationship.

For reference, this is the true relationship curve:
![[Screen Shot 2024-12-22 at 13.52.31 PM.png|400]]

### Approaching the problem:
1. split the data into two sets, one for training the algorithm and one for testing. The blue dots are the training data, and the green dots are for testing.
	![[Screen Shot 2024-12-22 at 13.54.13 PM.png|400]]

2. Considering only the training set, we use "Linear Regression" aka "Least Squares" as our first ML algorithm.
	![[Screen Shot 2024-12-22 at 13.56.31 PM.png|400]]
	- Linear regression fits a straight line to the training set.
	- This straight line does not have flexibility to accurately replicate the arc in the "true" relationship curve.
	- No matter how we may try to fit the line, it will never curve.
	- Thus, the straight line will never capture the "true" relationship between weight and height, no matter how well we fit the training set.
	- **This inability of a machine learning method (here linear regression) to capture the true relationship is called Bias.**
	- Because the straight line can't be curved like the true relationship, it has relatively large amounts of bias.

3. Alternatively, another ML method might fit a squiggly line to the training set. 
	![[Screen Shot 2024-12-22 at 14.02.35 PM.png|400]]
	- This squiggly line is super flexible and hugs the training set along the arc of the true relationship. 
	- Because the squiggly line can handle the arc in the true relationship between the weight and height, it has very little bias. 

### Comparing the two models
We compare how well the straight line (linear regression) and the squiggly line fit the training data by calculating their sum of squares.
![[Screen Shot 2024-12-22 at 14.07.53 PM.png]]

- In other words, we measure the distance from the fit lines (curve) to the data points, square them and add them up. We square them up so that the negative distances do not cancel out the positive distances.
- We see that the squiggly line fits the data so well that distance between the line and the data is 0. Therefore, the squiggly line performed better on the training set.

For the testing set,
![[Screen Shot 2024-12-22 at 14.11.23 PM.png]]

- The straight line performs better than the squiggly line for testing set.
- **The difference in fits between datasets is called [[Variance]].**

> [!Summary] In summary for this example
>- The squiggly line has low bias since it is flexible and can adapt to the curve in the relationship between weight and height, but has high variance because it results in vastly different sums of squares for different datasets.
>- In other words, its hard to predict how well the squiggly line will perform with future data sets.
>
>- In contrast, the straight line has relatively high bias, since it can not capture the curve in the relationship between weight and height, but has relatively low variance because the sums of squares are very similar for different datasets.
>- In other words, the straight line might not give great predictions and only give good predictions, but they will be consistently good predictions. 

The squiggly line performs well on training set and not on test set, therefore it is said to be [[overfitting]] the data. 

>[!Tip]
>The ideal ML model has low bias and low variance, which is done by finding the sweet spot between a simple model and a complex model. A too simple model will underfit the data, while a too complex model can overfit the data.

Commonly used methods to find this sweet spot between simple and complex models are:
- [[Regularization]]
- [[Boosting]]
- [[Bagging]]

Key takeaways:
- If the model is under fitting, high bias, low variance.
- If the model is overfitting, low bias, high variance.
- In machine learning, the ideal algorithm has low bias and can accurately model the true relationship, and has low variance by producing consistent predictions across different datasets.![[Screen Shot 2024-12-22 at 14.22.50 PM.png]]
