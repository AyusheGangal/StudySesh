Example, we want to use the variables (chest pain, good blood circulation, etc.,) to know if someone has heart disease. 
![[Screen Shot 2024-12-19 at 22.40.00 PM.png|400]]

Then if a new patient shows up, we can use this data to predict if that have a heart disease or not.
![[Screen Shot 2024-12-19 at 22.41.18 PM.png|400]]

- Cross validations allows us to compare different machine learning methods and get a sense of how well they will work in practice. 

Suppose we have X amount of data that represents all the data we have collected about people with and without heart disease. 

Now in machine learning, we need to perform two things with this data (majorly):
1. Estimate the parameters for the machine learning methods. For example, to use logistic regression, we have to use some of the data to estimate the shape of the curve. Estimating the parameters is also called a **"training the model/algorithm".**
2. Evaluate how well the machine learning methods work. For example, we need to find out if the curve will do a good job categorizing  new data. Evaluating a method is called **"testing the model/algorithm"**

If we divide our dataset X into 4 equal parts, we can use cross validation to use them (parts of data) all, one at a time, and summarize results at the end. In the end, every block of data is used for testing and we can compare methods by seeing how they performed. 

### Types of Cross Validation
- A cross validation technique where we divide the total data into 4 blocks. This is called Four-Fold cross validation (4-Fold)
- However, the number of blocks are arbitrary.
- In an extreme case, we could call each datapoint or sample (here, patient) a block. This technique is called **"Leave One Out Cross Validation" (LOO CV)**
- In practice, it is very common to divide the data into 10 blocks. This is called Ten-Fold cross validation.

### Use
Say, we wanted to use a method that involves "[[tuning]]" a parameter - which means a parameter that isn't estimated, but just sort of guessed. (example, ridge regression has a tuning parameter), then we can use ten-fold cross validation to help find the best value for this tuning parameter.

